{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pywikibot\n",
    "import queue\n",
    "import gensim\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # Suppresses gensim errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Things I've installed:\n",
    "1. wikipedia (query wikipedia) - not used\n",
    "2. pywikibot (more advanced queries)\n",
    "3. wikiutils (read sql.gz files) - not yet used. possibly in the future\n",
    "4. gensim (for NLP and specifically using Google's word2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load word2vec embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obviously override this to your local location\n",
    "model_addr = '/Users/benjaminrafetto/Code/cs182/project/data/GoogleNews-vectors-negative300.bin'\n",
    "\n",
    "# claire's address \n",
    "# model_addr = '/Users/clairestolz/CS182/CS182-Final-Project/data/GoogleNews-vectors-negative300.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format(model_addr, binary=True)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crude definition of distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Currently uses average word distances from word2vec embeddings\n",
    "def get_distance_new(topic, model, goal):\n",
    "    assert type(topic) is list and type(goal) is list\n",
    "    try:\n",
    "        distances = [model.distance(x, y) for x in topic for y in goal]\n",
    "        return (np.average(distances) + np.min(distances) / 2.0)  # Combination of average and minimum\n",
    "    except:\n",
    "        return np.Infinity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "site = pywikibot.Site(\"en\", \"wikipedia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_word2vec_path(start, goal, maxIters=30, verbose=False):\n",
    "    start_list = process_word(start)\n",
    "    goal_list = process_word(goal)\n",
    "    assert get_distance_new(start_list, model, goal_list) is not np.inf, \"Start and end nodes {} and {} must be in word2vec vocabulary.\".format(start, goal)\n",
    "    \n",
    "    start_page = pywikibot.Page(site, start)\n",
    "    path = []\n",
    "    visited = []\n",
    "    fringe = queue.PriorityQueue()\n",
    "    fringe.put((np.Inf, start_page))\n",
    "    \n",
    "    i = 0\n",
    "    while i < maxIters and not fringe.empty():\n",
    "        i += 1\n",
    "        priority, page = fringe.get()\n",
    "        path.append(page.title())\n",
    "        if verbose:\n",
    "            print(\"Exploring node {} with distance {}\".format(page.title(), priority))\n",
    "        if goal.lower() == page.title().lower():\n",
    "            return path\n",
    "\n",
    "        for p in page.linkedPages():\n",
    "            if p.title() not in visited:\n",
    "                visited.append(p.title())\n",
    "                processed = process_word(p.title())\n",
    "#                 print(p.title(), processed)\n",
    "                distance = get_distance_new(processed, model, goal_list)\n",
    "                fringe.put((distance, p))\n",
    "\n",
    "    return []\n",
    "#     raise Exception(\"Unable to find goal node.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_word(topic, model=model):\n",
    "    if not topic:  #No topic. Stop.\n",
    "        return []\n",
    "    \n",
    "    if topic in model:\n",
    "        return [topic]\n",
    "\n",
    "    output = []\n",
    "    words = topic.split(' ')\n",
    "\n",
    "    #Iteratively search word2vec for shorter and shorter phrases\n",
    "    for j in range(len(words), 1, -1):\n",
    "        test = '_'.join(words[:j])\n",
    "        if test in model:\n",
    "            return [test] + process_word(' '.join(words[j:]))\n",
    "\n",
    "    if words[0] in model:\n",
    "        output.append(words[0])\n",
    "    return output + process_word(' '.join(words[1:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [(\"speech\", \"lacrosse\"),\n",
    "            (\"mantra\", \"dna\"),\n",
    "            (\"Parthenon\", \"Natural Environment\"),\n",
    "            (\"Feces\", \"Poet\"),\n",
    "#             (\"penguin\", \"sans-serif\"),  #sans-serif is not in the dictionary\n",
    "            (\"angelina jolie\", \"nitrogen\"),\n",
    "            (\"Carrie Fisher\", \"Death of Adolf Hitler\"),\n",
    "            (\"Lacrosse\", \"Comedian\"),\n",
    "            (\"Dictionary\", \"Atmosphere of Earth\"),\n",
    "            (\"Broadway theatre\", \"Wall Street\"),\n",
    "            (\"Life expectancy\", \"Graphical User Interface\"),\n",
    "            (\"Diazepam\", \"Death\"),\n",
    "            (\"Moors\", \"Aryan\"),\n",
    "            (\"Michelangelo\", \"Horror Fiction\"),\n",
    "            (\"Jim Henson\", \"Gin\"),\n",
    "            (\"Continental Army\", \"Computer Multitasking\"),\n",
    "            (\"World Health Organization\", \"Ecosystem\"),\n",
    "            (\"Blood pressure\", \"Mathematics\"),\n",
    "            (\"War of 1812\", \"Queens of the Stone Age\"),\n",
    "            (\"Onomatopoeia\", \"Wiki\"),\n",
    "            (\"Church of England\", \"Joan Baez\"),\n",
    "            (\"Nuclear Power\", \"Canadians\"),\n",
    "            (\"Multi-sport event\", \"Ku Klux Klan\"),\n",
    "            (\"Pony Express\", \"Augustus\"),\n",
    "            (\"Organization\", \"Parthenon\"),\n",
    "            (\"Battleship\", \"Dream\"),\n",
    "            (\"The Cosby Show\", \"Marine biology\"),\n",
    "            (\"DNA replication\", \"Muscle car\"),\n",
    "            (\"Mammal\", \"Montreal\"),\n",
    "            (\"River\", \"Engine\"),\n",
    "            (\"Louis Armstrong\", \"Nuclear Power\"),\n",
    "            (\"Entertainment\", \"Ralph Waldo Emerson\"),\n",
    "            (\"Bilirubin\", \"Architecture\"),\n",
    "            (\"Association football\", \"Axis powers\"),\n",
    "            (\"World Series\", \"Nuclear warfare\")\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for shortest path from Carrie Fisher to Death of Adolf Hitler\n"
     ]
    }
   ],
   "source": [
    "start, goal = examples[np.random.choice(len(examples))]\n",
    "print(\"Searching for shortest path from {} to {}\".format(start,goal))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To-Do: exclude Wikipedia: Category: Talk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length 18 Path ['Organization', 'Enterprise architecture', 'Service-oriented architecture', 'Event-driven architecture', 'Space-based architecture', 'Talk:Event-driven architecture', 'Enterprise architecture artifacts', 'Software architecture', 'Database-centric architecture', 'Multi-tier architecture', 'Multitier architecture', 'Clientâ€“server architecture', 'Multilayered architecture', 'Structure', 'Chichen Itza', 'Maya civilization', 'Acropolis', 'Parthenon']\n"
     ]
    }
   ],
   "source": [
    "for start, goal in examples[-1:]:\n",
    "    path = greedy_word2vec_path(start, goal, maxIters=50, verbose=False)\n",
    "    print(\"Length\", len(path), \"Path\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
