{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pywikibot\n",
    "import queue\n",
    "import gensim\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # Suppresses gensim errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Things I've installed:\n",
    "1. wikipedia (query wikipedia) - not used\n",
    "2. pywikibot (more advanced queries)\n",
    "3. wikiutils (read sql.gz files) - not yet used. possibly in the future\n",
    "4. gensim (for NLP and specifically using Google's word2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load word2vec embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obviously override this to your local location\n",
    "model_addr = '/Users/benjaminrafetto/Code/cs182/project/data/GoogleNews-vectors-negative300.bin'\n",
    "\n",
    "# claire's address \n",
    "# model_addr = '/Users/clairestolz/CS182/CS182-Final-Project/data/GoogleNews-vectors-negative300.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format(model_addr, binary=True)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crude definition of distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Currently uses average word distances from word2vec embeddings\n",
    "def get_distance_new(topic, model, goal):\n",
    "    assert type(topic) is list and type(goal) is list\n",
    "    try:\n",
    "        distances = [model.distance(x, y) for x in topic for y in goal]\n",
    "        return (np.average(distances) + np.min(distances) / 2.0)  # Combination of average and minimum\n",
    "    except:\n",
    "        return np.Infinity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "site = pywikibot.Site(\"en\", \"wikipedia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_word2vec_path(start, goal, maxIters=30, verbose=False):\n",
    "    start_list = process_word(start)\n",
    "    goal_list = process_word(goal)\n",
    "    assert get_distance_new(start_list, model, goal_list) is not np.inf, \"Start and end nodes {} and {} must be in word2vec vocabulary.\".format(start, goal)\n",
    "    \n",
    "    start_page = pywikibot.Page(site, start)\n",
    "    path = []\n",
    "    visited = []\n",
    "    fringe = queue.PriorityQueue()\n",
    "    fringe.put((np.Inf, start_page))\n",
    "    \n",
    "    i = 0\n",
    "    while i < maxIters and not fringe.empty():\n",
    "        i += 1\n",
    "        priority, page = fringe.get()\n",
    "        path.append(page.title())\n",
    "        if verbose:\n",
    "            print(\"Exploring node {} with distance {}\".format(page.title(), priority))\n",
    "        if goal.lower() == page.title().lower():\n",
    "            return path\n",
    "\n",
    "        for p in page.linkedPages():\n",
    "            if p.title() not in visited:\n",
    "                visited.append(p.title())\n",
    "                processed = process_word(p.title())\n",
    "#                 print(p.title(), processed)\n",
    "                distance = get_distance_new(processed, model, goal_list)\n",
    "                fringe.put((distance, p))\n",
    "\n",
    "    raise Exception(\"Unable to find goal node.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some example paths. Currently only supports start and goal nodes that are specifically in word2vec.\n",
    "i.e. Sentences don't work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = \"Natural Environment\"\n",
    "t2 = \"Angelina Jolie\"\n",
    "t3 = \"Carrie Fisher\"\n",
    "t4 = \"Death of Adolf Hitler\"\n",
    "t5 = t4.split(' ')\n",
    "test = [t5[:1], t5[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [(\"speech\", \"lacrosse\"),\n",
    "            (\"mantra\", \"dna\"),\n",
    "            (\"Parthenon\", \"Natural Environment\"),\n",
    "            (\"Feces\", \"Poet\"),\n",
    "#             (\"penguin\", \"sans-serif\"),  #sans-serif is not in the dictionary\n",
    "            (\"angelina jolie\", \"nitrogen\"),\n",
    "            (\"Carrie Fisher\", \"Death of Adolf Hitler\"),\n",
    "            (\"Lacrosse\", \"Comedian\"),\n",
    "            (\"Dictionary\", \"Atmosphere of Earth\"),\n",
    "            (\"Broadway theatre\", \"Wall Street\"),\n",
    "            (\"Life expectancy\", \"Graphical User Interface\"),\n",
    "            (\"Diazepam\", \"Death\"),\n",
    "            (\"Moors\", \"Aryan\"),\n",
    "            (\"Michelangelo\", \"Horror Fiction\"),\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for shortest path from Parthenon to Natural Environment\n"
     ]
    }
   ],
   "source": [
    "start, goal = examples[np.random.choice(len(examples))]\n",
    "print(\"Searching for shortest path from {} to {}\".format(start,goal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting at 1 ['Atmosphere', 'of Earth']\n",
      "Processed [['Atmosphere'], ['of', 'Earth']]\n",
      "Splitting at 2 ['Atmosphere of', 'Earth']\n",
      "Processed [['Atmosphere', 'of'], ['Earth']]\n",
      "Combinations [[['Atmosphere'], ['of', 'Earth']], [['Atmosphere', 'of'], ['Earth']]]\n",
      "Best [['Atmosphere'], ['of', 'Earth']]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Start and end nodes Dictionary and Atmosphere of Earth must be in word2vec vocabulary.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-105-4400c5df733c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgoal\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgreedy_word2vec_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgoal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxIters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-27-17f0d4f983a1>\u001b[0m in \u001b[0;36mgreedy_word2vec_path\u001b[0;34m(start, goal, maxIters, verbose)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mstart_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mgoal_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgoal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mget_distance_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgoal_list\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Start and end nodes {} and {} must be in word2vec vocabulary.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgoal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mstart_page\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpywikibot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msite\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Start and end nodes Dictionary and Atmosphere of Earth must be in word2vec vocabulary."
     ]
    }
   ],
   "source": [
    "for start, goal in examples[-5:]:\n",
    "    print(greedy_word2vec_path(start, goal, maxIters=50, verbose=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hideously ugly code.\n",
    "\n",
    "def process_word(topic, model=model):\n",
    "    return unembed(process_word_rec(topic, model))\n",
    "\n",
    "def unembed(list_of_lists):\n",
    "    results = []\n",
    "    for l in list_of_lists:\n",
    "        if type(l) is list:\n",
    "            results += [l2 for l2 in unembed(l)]\n",
    "        else:\n",
    "            results.append(l)\n",
    "    return results\n",
    "\n",
    "def process_word_rec(topic, model):\n",
    "    if topic in model:\n",
    "        return [topic]\n",
    "    if topic.replace(' ', '_') in model:\n",
    "        return [topic.replace(' ', '_')]\n",
    "    # Recursively split phrases into longest subphrases that are in our model\n",
    "    words = topic.split(' ')\n",
    "    if len(words) == 1:\n",
    "        return []\n",
    "    if len(words) == 2:\n",
    "        return words\n",
    "    \n",
    "    combinations = []\n",
    "    # If too long for the exponential approach just split in half:\n",
    "    split_range = [len(words) // 2] if len(words) > 6 else range(1, len(words))\n",
    "    for i in split_range:\n",
    "        two = [' '.join(w) for w in [words[:i], words[i:]]]\n",
    "        print(\"Splitting at {}\".format(i), two)\n",
    "        two_processed = [process_word_rec(t, model) for t in two]\n",
    "        combinations.append(two_processed)\n",
    "        print(\"Processed\", two_processed)\n",
    "    \n",
    "    best = min(combinations, key=lambda x: len(x))\n",
    "    print(\"Combinations\", combinations)\n",
    "    print(\"Best\", best)\n",
    "    return min(combinations, key=lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_word_rec(topic, model=model):\n",
    "#     print(\"Topic:\", topic)\n",
    "    if not topic:  #No topic. Stop.\n",
    "        return []\n",
    "    \n",
    "    if topic in model:\n",
    "        return [topic]\n",
    "\n",
    "    output = []\n",
    "    words = topic.split(' ')\n",
    "\n",
    "    for j in range(len(words), 1, -1):\n",
    "        test = '_'.join(words[:j])\n",
    "        if test in model:\n",
    "            return [test] + process_word_rec(' '.join(words[j:]))\n",
    "\n",
    "    if words[0] in model:\n",
    "        output.append(words[0])\n",
    "    return output + process_word_rec(' '.join(words[1:]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
