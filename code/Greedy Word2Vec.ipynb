{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pywikibot\n",
    "import queue\n",
    "import gensim\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # Suppresses gensim errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Things I've installed:\n",
    "1. wikipedia (query wikipedia) - not used\n",
    "2. pywikibot (more advanced queries)\n",
    "3. wikiutils (read sql.gz files) - not yet used. possibly in the future\n",
    "4. gensim (for NLP and specifically using Google's word2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load word2vec embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obviously override this to your local location\n",
    "#model_addr = '/Users/benjaminrafetto/Code/cs182/project/data/GoogleNews-vectors-negative300.bin'\n",
    "\n",
    "# claire's address \n",
    "model_addr = '/Users/clairestolz/CS182/CS182-Final-Project/data/GoogleNews-vectors-negative300.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format(model_addr, binary=True)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crude definition of distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Currently uses average word distances from word2vec embeddings\n",
    "def get_distance(topic, model, goal):\n",
    "    if topic in model:\n",
    "        return model.distance(goal, topic)\n",
    "    else:\n",
    "        distances = [model.distance(goal, w) for w in topic.split(' ') if w in model]\n",
    "        return np.average(distances) if distances else np.Infinity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "site = pywikibot.Site(\"en\", \"wikipedia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_word2vec_path(start, goal, maxIters=30, verbose=False):\n",
    "    assert start in model and goal in model, \"Start and end nodes {} and {} must be in word2vec vocabulary.\".format(start,goal)\n",
    "    start_page = pywikibot.Page(site, start)\n",
    "    path = []\n",
    "    visited = []\n",
    "    fringe = queue.PriorityQueue()\n",
    "    fringe.put((np.Inf, start_page))\n",
    "    \n",
    "    i = 0\n",
    "    while i < maxIters and not fringe.empty():\n",
    "        i += 1\n",
    "        priority, page = fringe.get()\n",
    "        path.append(page.title())\n",
    "        if verbose:\n",
    "            print(\"Exploring node {} with distance {}\".format(page.title(), priority))\n",
    "        if goal.lower() == page.title().lower():\n",
    "            return path\n",
    "\n",
    "        for p in page.linkedPages():\n",
    "            if p.title() not in visited:\n",
    "                visited.append(p.title())\n",
    "                distance = get_distance(p.title(), model, goal)\n",
    "                fringe.put((distance, p))\n",
    "\n",
    "    raise Exception(\"Unable to find goal node.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some example paths. Currently only supports start and goal nodes that are specifically in word2vec.\n",
    "i.e. Sentences don't work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [(\"speech\", \"lacrosse\"),\n",
    "            (\"mantra\", \"dna\"),\n",
    "            (\"Parthenon\", \"Environment\"),  #\"Natural Environment\"\n",
    "            (\"Feces\", \"Poet\")\n",
    "#             (\"penguin\", \"sans-serif\"),\n",
    "#             (\"angelina jolie\", \"nitrogen\"),\n",
    "#             (\"Carrie Fisher\", \"Death of Adolf Hitler\"),\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for shortest path from speech to lacrosse\n"
     ]
    }
   ],
   "source": [
    "start, goal = examples[np.random.choice(len(examples))]\n",
    "print(\"Searching for shortest path from {} to {}\".format(start,goal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Speech',\n",
       " 'Grammar',\n",
       " 'Grammar school',\n",
       " 'Yeshiva',\n",
       " 'One-room school',\n",
       " 'State-integrated school',\n",
       " 'Ungraded school',\n",
       " 'University-preparatory school',\n",
       " 'College-preparatory school',\n",
       " 'College Prep',\n",
       " 'Talk:Ungraded school',\n",
       " 'College',\n",
       " 'College football',\n",
       " 'College lacrosse',\n",
       " 'Lacrosse']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "greedy_word2vec_path(start, goal, maxIters=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BFS search to get a baseline for performance \n",
    "def bfs_wiki(start, goal, maxDepth=30, verbose=False):\n",
    "    assert start in model and goal in model, \"Start and end nodes {} and {} must be in word2vec vocabulary.\".format(start,goal)\n",
    "    start_page = pywikibot.Page(site, start)\n",
    "    path = []\n",
    "    visited = []\n",
    "    fringe = queue.PriorityQueue()\n",
    "    fringe.put((np.Inf, start_page))\n",
    "    \n",
    "    i = 0\n",
    "    while i < maxIters and not fringe.empty():\n",
    "        i += 1\n",
    "        priority, page = fringe.get()\n",
    "        path.append(page.title())\n",
    "        if verbose:\n",
    "            print(\"Exploring node {} with distance {}\".format(page.title(), priority))\n",
    "        if goal.lower() == page.title().lower():\n",
    "            return path\n",
    "\n",
    "        for p in page.linkedPages():\n",
    "            if p.title() not in visited:\n",
    "                visited.append(p.title())\n",
    "                distance = get_distance(p.title(), model, goal)\n",
    "                fringe.put((distance, p))\n",
    "\n",
    "    raise Exception(\"Unable to find goal node.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
