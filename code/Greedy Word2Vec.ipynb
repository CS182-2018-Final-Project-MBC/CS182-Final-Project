{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pywikibot\n",
    "import queue\n",
    "import gensim\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # Suppresses gensim errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Things I've installed:\n",
    "1. wikipedia (query wikipedia) - not used\n",
    "2. pywikibot (more advanced queries)\n",
    "3. wikiutils (read sql.gz files) - not yet used. possibly in the future\n",
    "4. gensim (for NLP and specifically using Google's word2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load word2vec embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obviously override this to your local location\n",
    "model_addr = '/Users/benjaminrafetto/Code/cs182/project/data/GoogleNews-vectors-negative300.bin'\n",
    "\n",
    "# claire's address \n",
    "# model_addr = '/Users/clairestolz/CS182/CS182-Final-Project/data/GoogleNews-vectors-negative300.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format(model_addr, binary=True)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crude definition of distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Currently uses average word distances from word2vec embeddings\n",
    "def get_distance_new(topic, model, goal):\n",
    "    assert type(topic) is list and type(goal) is list\n",
    "    try:\n",
    "        distances = [model.distance(x, y) for x in topic for y in goal]\n",
    "        return (np.average(distances) + np.min(distances) / 2.0)  # Combination of average and minimum\n",
    "    except:\n",
    "        return np.Infinity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "site = pywikibot.Site(\"en\", \"wikipedia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_word2vec_path(start, goal, maxIters=30, verbose=False):\n",
    "    start_list = process_word(start)\n",
    "    goal_list = process_word(goal)\n",
    "    assert get_distance_new(start_list, model, goal_list) is not np.inf, \"Start and end nodes {} and {} must be in word2vec vocabulary.\".format(start, goal)\n",
    "    \n",
    "    start_page = pywikibot.Page(site, start)\n",
    "    path = []\n",
    "    visited = []\n",
    "    fringe = queue.PriorityQueue()\n",
    "    fringe.put((np.Inf, start_page))\n",
    "    \n",
    "    i = 0\n",
    "    while i < maxIters and not fringe.empty():\n",
    "        i += 1\n",
    "        priority, page = fringe.get()\n",
    "        path.append(page.title())\n",
    "        if verbose:\n",
    "            print(\"Exploring node {} with distance {}\".format(page.title(), priority))\n",
    "        if goal.lower() == page.title().lower():\n",
    "            return path\n",
    "\n",
    "        for p in page.linkedPages():\n",
    "            if p.title() not in visited:\n",
    "                visited.append(p.title())\n",
    "                processed = process_word(p.title())\n",
    "#                 print(p.title(), processed)\n",
    "                distance = get_distance_new(processed, model, goal_list)\n",
    "                fringe.put((distance, p))\n",
    "\n",
    "    raise Exception(\"Unable to find goal node.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_word(topic, model=model):\n",
    "    if not topic:  #No topic. Stop.\n",
    "        return []\n",
    "    \n",
    "    if topic in model:\n",
    "        return [topic]\n",
    "\n",
    "    output = []\n",
    "    words = topic.split(' ')\n",
    "\n",
    "    #Iteratively search word2vec for shorter and shorter phrases\n",
    "    for j in range(len(words), 1, -1):\n",
    "        test = '_'.join(words[:j])\n",
    "        if test in model:\n",
    "            return [test] + process_word(' '.join(words[j:]))\n",
    "\n",
    "    if words[0] in model:\n",
    "        output.append(words[0])\n",
    "    return output + process_word(' '.join(words[1:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [(\"speech\", \"lacrosse\"),\n",
    "            (\"mantra\", \"dna\"),\n",
    "            (\"Parthenon\", \"Natural Environment\"),\n",
    "            (\"Feces\", \"Poet\"),\n",
    "#             (\"penguin\", \"sans-serif\"),  #sans-serif is not in the dictionary\n",
    "            (\"angelina jolie\", \"nitrogen\"),\n",
    "            (\"Carrie Fisher\", \"Death of Adolf Hitler\"),\n",
    "            (\"Lacrosse\", \"Comedian\"),\n",
    "            (\"Dictionary\", \"Atmosphere of Earth\"),\n",
    "            (\"Broadway theatre\", \"Wall Street\"),\n",
    "            (\"Life expectancy\", \"Graphical User Interface\"),\n",
    "            (\"Diazepam\", \"Death\"),\n",
    "            (\"Moors\", \"Aryan\"),\n",
    "            (\"Michelangelo\", \"Horror Fiction\"),\n",
    "            (\"Jim Henson\", \"Gin\"),\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for shortest path from Diazepam to Death\n"
     ]
    }
   ],
   "source": [
    "start, goal = examples[np.random.choice(len(examples))]\n",
    "print(\"Searching for shortest path from {} to {}\".format(start,goal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length 35 Path ['Jim Henson', 'Faygo', 'Carbonated', 'Carbonation', 'Sabai Sabai Sesame', 'Zhima Jie', 'Coffee Talk', 'Grape', 'Juice', 'Juice (disambiguation)', 'Gin and Juice', 'Gin & Juice (DeVante Swing song)', 'Gin & Juice', 'Juice (American band)', 'Juice (American rapper)', 'Juice (Australian band)', \"Juice (B'z song)\", 'Juice (Danish band)', 'Juice (German magazine)', 'Juice (JVM)', 'Juice (Serbian rapper)', 'Juice (aggregator)', 'Juice (film)', 'Juice (skateboarding magazine)', 'Juice (soundtrack)', 'Juice (trio)', 'Talk:Gin and Juice', 'Raspberry juice', 'Raspberry', 'Raspberry (disambiguation)', 'Raspberry (color)', 'Raspberry (song)', 'Framboise', 'Beer', 'Gin']\n"
     ]
    }
   ],
   "source": [
    "for start, goal in examples[-1:]:\n",
    "    path = greedy_word2vec_path(start, goal, maxIters=50, verbose=False)\n",
    "    print(\"Length\", len(path), \"Path\", path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
